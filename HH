import requests
import fake_useragent
from bs4 import BeautifulSoup
import time


def get_links():
    ua =fake_useragent.UserAgent()
    res = requests.get(
        url="https://hh.ru/search/vacancy?text=&area=1500&page=0",
        headers={"user-agent":ua.random}
    )
    print(res.status_code)

    soup = BeautifulSoup(res.content, "lxml")
    try:
        page_count = int(soup.find("div",attrs={"class":"pager"}).find_all("span",recursive=False)[-1].find("a").find("span").text)
    except:
        return
    print(page_count)
    for page in range(0,page_count+1):
      for i in range(100):
        res = requests.get(
        url=f"https://hh.ru/search/vacancy?area=1500&industry={i}&page={page}",     #2 цикла. сначала листает страницу, потом перебирает отрасли(industry)
          headers={"user-agent":ua.random}
          )
        soup = BeautifulSoup(res.content, "lxml")
        for a in soup.find_all('a',class_="serp-item__title"):
          yield f'{a.attrs["href"].split("?")[0]}'      #Создается список из ссылок
          print(a.attrs['href'])
          time.sleep(1)
        

def get_info(link):
  ua =fake_useragent.UserAgent()
  res = requests.get(
      url=link,
      headers={"user-agent":ua.random}
  )
  soup = BeautifulSoup(res.content, 'lxml')
  try:
    name = soup.find('h1',class_='bloko-header-section-1').text
  except:
    name = 'Не указано'
  try:
    salary_gross = soup.find(attrs={'data-qa':'vacancy-salary-compensation-type-gross'}).text.replace('&nbsp;','').replace("\xa0","")
  except:
    salary_gross = ''

  try:
    salary_undefined = soup.find(attrs={'data-qa':'vacancy-salary-compensation-type-undefined'}).text.replace('&nbsp;','').replace("\xa0","")
  except:
    salary_undefined = ''
  try:
    salary_net = soup.find(attrs={'data-qa':'vacancy-salary-compensation-type-net'}).text.replace('&nbsp;','').replace("\xa0","")

  except:
    salary_net = ''
  salary = salary_gross + salary_net + salary_undefined
  if salary == '':
    salary = 'з/п не указана'
  try:
    work_exp = soup.find(attrs={'data-qa':'vacancy-experience'}).text
  except:
    work_exp = ''
  try:
    company_name = soup.find(attrs={'data-qa':'vacancy-company-name'}).text.replace("\xa0","")
  except:
    company_name = ''
  try:
    company_location = soup.find(attrs={'data-qa':'vacancy-view-raw-address'}).text
  except:
    company_location = soup.find(attrs={'data-qa':'vacancy-view-location'}).text

  try:
    tags = [tag.text for tag in soup.find(attrs={'class':'bloko-tag-list'}).find_all(attrs={'class':'bloko-tag__section bloko-tag__section_text'})]
    tags = ', '.join(tags)
  except:
    tags = []
  full_data = [name, salary, work_exp, company_name, company_location, tags]        #Список из значений
 
  return full_data


data = [['Название вакансии', 'Зарплата', 'Необходимый опыт', 'Название компании', 'Локация', 'Ключевые навыки']]
k = 0
for a in get_links():
  data.append(get_info(a))      #Добавляем в общий список
  time.sleep(1)
  k+= 1
  print(k)

with open('hh.csv','w', encoding='utf-8') as f:         #Запишем в файл
    fq = csv.writer(f, delimiter=',', dialect='excel')
    #fq = csv.DictWriter(f, vac.keys())
    #fq.writeheader()
    #fq.writerow(vac)
    for a in data:
      fq.writerow(a)

print('end', k)
